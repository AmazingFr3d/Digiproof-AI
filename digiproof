{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11520209,"sourceType":"datasetVersion","datasetId":7224979}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis project aims to develop a sophisticated machine learning model designed to effectively match job offers with suitable candidates based on their specific skills and qualifications. I leveraged the RandomForestClassifier, a robust and versatile algorithm from the scikit-learn library in Python, to construct a predictive model. This model examines a range of attributes from both job descriptions, such as required skills, salary, job location, and years of experience, and candidate profiles, including their prior work experience and skill sets.\n\nFor the data preparation and manipulation phases, I utilised the powerful pandas library. This tool facilitated a systematic approach to organising, cleaning, and transforming the raw datasets, which is crucial for ensuring high-quality input for the model. The meticulous data preparation process significantly enhanced the modelâ€™s accuracy during training and evaluation.\n\nGiven the constraints of the project timeline, the dataset employed in this project is synthetic, generated through the code snippet provided below. This synthetic dataset serves as a stand-in, allowing for preliminary testing and validation. Once a real dataset becomes available, it can be seamlessly integrated into the model for future enhancements and more realistic performance assessments.","metadata":{}},{"cell_type":"code","source":"%%script false --no-raise-error # Remove this line to run this cell \n!pip install -U scikit-learn imbalanced-learn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:03:11.165981Z","iopub.execute_input":"2025-04-24T00:03:11.166748Z","iopub.status.idle":"2025-04-24T00:03:11.180367Z","shell.execute_reply.started":"2025-04-24T00:03:11.166718Z","shell.execute_reply":"2025-04-24T00:03:11.179376Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:03:11.182550Z","iopub.execute_input":"2025-04-24T00:03:11.182953Z","iopub.status.idle":"2025-04-24T00:03:13.688714Z","shell.execute_reply.started":"2025-04-24T00:03:11.182927Z","shell.execute_reply":"2025-04-24T00:03:13.687510Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Dataset Generation\nI will generates some random data set for candodates and jobs with the code bellow as time to not parmit me to gather real world data. This dataset can easyly be swapped in future after some extra preprocessing steps.","metadata":{}},{"cell_type":"code","source":"# Convert to DataFrame\ndef pretty_report(report):\n    df_report = pd.DataFrame(report).transpose()\n    \n    # If accuracy is a single number, turn it into a row for display consistency\n    if 'accuracy' in df_report.index:\n        acc = df_report.loc['accuracy'].values[0] if isinstance(df_report.loc['accuracy'], pd.Series) else df_report.loc['accuracy']\n        df_report.loc['accuracy'] = [None, None, acc, None]\n    \n    # Display the table\n    print(df_report.fillna(\"\").round(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:03:13.689902Z","iopub.execute_input":"2025-04-24T00:03:13.690552Z","iopub.status.idle":"2025-04-24T00:03:13.697390Z","shell.execute_reply.started":"2025-04-24T00:03:13.690500Z","shell.execute_reply":"2025-04-24T00:03:13.695844Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%%script false --no-raise-error # Remove this line to run this cell \n\n# Pools for synthetic data generation\nskills_pool = [\"Python\", \"Java\", \"SQL\", \"React\", \"AWS\", \"Django\", \"Data Analysis\", \"Machine Learning\", \"Node.js\", \"Kubernetes\"]\nlocations = [\"New York\", \"Remote\", \"San Francisco\", \"Austin\", \"Chicago\", \"Seattle\"]\n\n# Generate synthetic candidates\ndef generate_candidate(i):\n    return {\n        \"candidate_id\": f\"C{i:04d}\",\n        \"name\": f\"Candidate_{i}\",\n        \"skills\": \", \".join(random.sample(skills_pool, k=3)),\n        \"experience_years\": random.randint(1, 15),\n        \"preferred_location\": random.choice(locations),\n        \"expected_salary\": random.randint(50000, 150000)\n    }\n\n# Generate synthetic jobs\ndef generate_job(i):\n    min_salary = random.randint(60000, 90000)\n    max_salary = min_salary + random.randint(10000, 40000)\n    return {\n        \"job_id\": f\"J{i:04d}\",\n        \"title\": random.choice([\"Software Engineer\", \"Data Scientist\", \"Web Developer\", \"DevOps Engineer\"]),\n        \"description\": \"We are looking for someone skilled in \" + \", \".join(random.sample(skills_pool, k=3)) + \".\",\n        \"required_skills\": \", \".join(random.sample(skills_pool, k=3)),\n        \"location\": random.choice(locations),\n        \"salary_min\": min_salary,\n        \"salary_max\": max_salary\n    }\n\n# Create datasets\ncandidates = [generate_candidate(i) for i in range(1, 10001)]\njobs = [generate_job(i) for i in range(1, 5000)]\n\n# Convert to DataFrames\ncandidates_df = pd.DataFrame(candidates)\njobs_df = pd.DataFrame(jobs)\n\n# Save to CSV\ncandidates_path = \"/kaggle/working/synthetic_candidates.csv\"\njobs_path = \"/kaggle/working/synthetic_jobs.csv\"\ncandidates_df.to_csv(candidates_path, index=False)\njobs_df.to_csv(jobs_path, index=False)\n\ncandidates_path, jobs_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:03:13.699227Z","iopub.execute_input":"2025-04-24T00:03:13.699533Z","iopub.status.idle":"2025-04-24T00:03:13.742423Z","shell.execute_reply.started":"2025-04-24T00:03:13.699508Z","shell.execute_reply":"2025-04-24T00:03:13.741474Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Preprocessing the Data (Candidates + Jobs\n\nI this section I will process the candidate and jobs datasets to turn the variables into numerical vectors that I can use with scikit-learn.\n\nI will do the following:\n\n- Tokenize and vectorize the text variables\n- Encode categorical variables\n- Normalize the numerical variable","metadata":{}},{"cell_type":"markdown","source":"## Loading The datasets","metadata":{}},{"cell_type":"code","source":"candidates_df = pd.read_csv(\"/kaggle/input/dummy-job-data/synthetic_candidates.csv\")\njobs_df = pd.read_csv(\"/kaggle/input/dummy-job-data/synthetic_jobs.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:03:13.743595Z","iopub.execute_input":"2025-04-24T00:03:13.743942Z","iopub.status.idle":"2025-04-24T00:03:13.836979Z","shell.execute_reply.started":"2025-04-24T00:03:13.743907Z","shell.execute_reply":"2025-04-24T00:03:13.835392Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"candidates_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:03:13.838171Z","iopub.execute_input":"2025-04-24T00:03:13.838591Z","iopub.status.idle":"2025-04-24T00:03:13.871591Z","shell.execute_reply.started":"2025-04-24T00:03:13.838486Z","shell.execute_reply":"2025-04-24T00:03:13.870602Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 6 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   candidate_id        10000 non-null  object\n 1   name                10000 non-null  object\n 2   skills              10000 non-null  object\n 3   experience_years    10000 non-null  int64 \n 4   preferred_location  10000 non-null  object\n 5   expected_salary     10000 non-null  int64 \ndtypes: int64(2), object(4)\nmemory usage: 468.9+ KB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"jobs_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:03:13.872809Z","iopub.execute_input":"2025-04-24T00:03:13.873162Z","iopub.status.idle":"2025-04-24T00:03:13.891843Z","shell.execute_reply.started":"2025-04-24T00:03:13.873129Z","shell.execute_reply":"2025-04-24T00:03:13.890968Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4999 entries, 0 to 4998\nData columns (total 7 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   job_id           4999 non-null   object\n 1   title            4999 non-null   object\n 2   description      4999 non-null   object\n 3   required_skills  4999 non-null   object\n 4   location         4999 non-null   object\n 5   salary_min       4999 non-null   int64 \n 6   salary_max       4999 non-null   int64 \ndtypes: int64(2), object(5)\nmemory usage: 273.5+ KB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Create Matched Pairs (Features + Labels)\n\nI will the the following to create a supervised ML model:\n\n- Inputs: combined features in this case job + candidate\n- Label: (1) for good match and (0) for not\n\nSince this data set was a synthetic data set generated by one of my python code, I will be using a rule based labelling to create our target variable by defining a simple matching rules based on overlapping skills, salaries, and so on.","metadata":{}},{"cell_type":"code","source":"def match_pairs_gen(jobs, candidates, num_pairs=1000):\n    data = []\n    for _ in range(num_pairs):\n        job = jobs.sample(1).iloc[0]\n        candidate = candidates.sample(1).iloc[0]\n        label = int(any(skill.strip() in candidate['skills'] for skill in job['required_skills'].split(',')))\n        data.append({\n            \"job_id\": job['job_id'],\n            \"candidate_id\": candidate['candidate_id'],\n            \"job_description\": job['description'],\n            \"job_required_skills\": job['required_skills'],\n            \"job_location\": job['location'],\n            \"job_salary_min\": job['salary_min'],\n            \"job_salary_max\": job['salary_max'],\n            \"candidate_skills\": candidate['skills'],\n            \"experience_years\": candidate['experience_years'],\n            \"preferred_location\": candidate['preferred_location'],\n            \"expected_salary\": candidate['expected_salary'],\n            \"label\": label\n            \n        })\n    return pd.DataFrame(data)\n\nmatch_df = match_pairs_gen(jobs_df, candidates_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:03:13.892883Z","iopub.execute_input":"2025-04-24T00:03:13.893212Z","iopub.status.idle":"2025-04-24T00:03:14.722049Z","shell.execute_reply.started":"2025-04-24T00:03:13.893181Z","shell.execute_reply":"2025-04-24T00:03:14.721077Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Preprocessing Pipeline\nI will use the ColumnTransformer to handle the various type of columns. at this step i will vectorize and normalize where neccesary","metadata":{}},{"cell_type":"code","source":"# Defining features and label\nX = match_df.drop(columns=[\"label\", \"job_id\", \"candidate_id\"])\ny = match_df[\"label\"]\n\n\n\n# Columns to preprocess\ntext_features = [\"job_description\", \"job_required_skills\", \"candidate_skills\"]\ncategorical_features = [\"job_location\", \"preferred_location\"]\nnumeric_features = [\"job_salary_min\", \"job_salary_max\", \"experience_years\", \"expected_salary\"]\n\n# Preprocessing of each column type\npreprocessor = ColumnTransformer([\n    (\"text\", TfidfVectorizer(), \"job_description\"),\n    (\"skills_job\", TfidfVectorizer(), \"job_required_skills\"),\n    (\"skills_candidate\", TfidfVectorizer(), \"candidate_skills\"),\n    (\"cat\", OneHotEncoder(), categorical_features),\n    (\"num\", StandardScaler(), numeric_features)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:03:14.724586Z","iopub.execute_input":"2025-04-24T00:03:14.724874Z","iopub.status.idle":"2025-04-24T00:03:14.732587Z","shell.execute_reply.started":"2025-04-24T00:03:14.724851Z","shell.execute_reply":"2025-04-24T00:03:14.731236Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Full Model Pipeline\n\nI will be using the RandomForestClassifier as it is easy to use and handle mixed data types.","metadata":{}},{"cell_type":"code","source":"# Full pipeline\npipeline = Pipeline([\n    (\"preprocessor\", preprocessor),\n    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n])\n\n# Train and test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Training the pipeline\npipeline.fit(X_train, y_train)\n\n#Prediction and evaluation\ny_pred = pipeline.predict(X_test)\nreport = classification_report(y_test, y_pred, output_dict=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:03:14.733671Z","iopub.execute_input":"2025-04-24T00:03:14.734011Z","iopub.status.idle":"2025-04-24T00:03:15.133742Z","shell.execute_reply.started":"2025-04-24T00:03:14.733984Z","shell.execute_reply":"2025-04-24T00:03:15.132544Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Model Performance\n\n- According to the report bellow this model is very good at idetifying actual matches hence the high recall for class (1).\n- It is less confident in identifying non-matches and some bad matches might still sneak through hence the lower recall value for class (0).\n- The overall F1 score lookes good consindering the fact our data is synthetic.","metadata":{}},{"cell_type":"code","source":"pretty_report(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T00:03:15.134889Z","iopub.execute_input":"2025-04-24T00:03:15.135311Z","iopub.status.idle":"2025-04-24T00:03:15.150850Z","shell.execute_reply.started":"2025-04-24T00:03:15.135280Z","shell.execute_reply":"2025-04-24T00:03:15.149936Z"}},"outputs":[{"name":"stdout","text":"             precision    recall  f1-score support\n0             0.571429  0.063492     0.114    63.0\n1             0.694301  0.978102     0.812   137.0\naccuracy                             0.690        \nmacro avg     0.632865  0.520797     0.463   200.0\nweighted avg  0.655596      0.69     0.592   200.0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Model improvement\n\nI will do the following to improve the model performance:\n\n- Balance the Dataset\n- Enhance the Features\n- Hyperparameter Tuning\n- Export the Pipeline\n- Recommender Function","metadata":{}},{"cell_type":"markdown","source":"## Balance the Dataset","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}